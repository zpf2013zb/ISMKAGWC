%ALGORITHM
Considering the inherent complexity of NP-hard problem, we design two approximation algorithms, namely CubeTree and MaxMargin with provable approximation ratio in Sections 5.1 and 5.2 respectively. Besides, we also propose the exact algorithm MergeList in Section 5.3.

\subsection{Approximation Algorithm MaxMargin}
In section 5.1 we show how to utilize the CubeTree algorithm to handle our problem. Although CubeTree can solve our problem with provable approximation ratio, however, as presented in Theorem 2, the approximation ratio of CubeTree is unstable due to the relevance with final solution. In this section, we elaborate the MaxMargin algorithm, which is inspired by the greedy strategy adopted by the WSC problem \cite{chvatal1979greedy}, which with more stable performance in terms of approximation ratio. To address WSC problem, the greedy strategy iteratively selects the current optimal subset and updates subsets that have not been visited yet accordingly. We modify this strategy by iteratively selecting the object with maximum $cr(o,q)$, and updating objects that have not been visited for our problem. The naive version of MaxMargin solves the problem by four steps. We use G to reserve the solution.
\begin{itemize}
    \item Step 1 (Construct the $RO_q$): Construct the relevant set $RO_q$ with KHT and sort the objects of $RO_q$ in descending order of $cr(o,q)$.
    \item Step 2 (Select the optimal object): In this step, MaxMargin adds the current optimal object o with maximum $cr(o,q)$ into G. Then, o is deleted from $RO_q$.
    \item Step 3 (Update $RO_q$): After adding o into G, we update the $cr(o,q)$ of remaining objects in $RO_q$.
    \item Step 4 (Iterative step): Repeat Step 2 and Step 3 until the weight constraint in definition 4 is satisfied. Then we take G as our final solution.
\end{itemize}

Obviously, two major drawbacks degrade the performance of naive version: 1) lacking of efficient pruning strategies; 2) updating all the remaining objects for each iteration in Step 3 is time consuming. To further boost the search efficiency, we take a look at several notions which settle aforementioned drawbacks collectively.

\begin{definition}
    \textbf{(Keyword Max Priority Queue).}: For a given keyword $\lambda$, we define the Keyword Max Priority Queue (KMPQ) of $\lambda$ as the max priority queue of $RO_\lambda$ according to the value of $cr(o,q)$.
\end{definition}

Each element of KMPQ of the form $(nid,oid,cr,cv)$, where nid and oid correspond to the identifier of element and the object, respectively. $cr$ represents the contribution ratio of oid and cv is a $|q.\omega|$ dimensions vector to record the $cw(o,\lambda)$ for each keyword in $q.\omega$. Instead of constructing the $RO_q$, we construct the KMPQ for each query keyword. Once $cov(G,\lambda)$ reaches $q.\theta$, we can prune objects in the KMPQ of $\lambda$ safely, which significantly exalts the performance.

To address the second drawback, we expand the notion of contribution ratio to fit in with our algorithm.

\begin{definition}
    \textbf{(Dynamic contribution ratio).}: Given a query q and an object o, we define the dynamic contribution ratio (dcr) of object o in iteration r as follows:
    \begin{equation}
        dcr_{q}^{r}(o)=\frac{cov^r(o,q)}{cd(o,q)}
    \end{equation}
\end{definition}

Note that the major difference between $dcr_{q}^{r}(o)$ and $cr(o,q)$ is that, we replace $cov(o,q)$ with $cov^r(o,q)$, where $cov^r(o,q)$ denotes the last contribution ratio of o after $r$th object added into result set. We do not update $dcr$ for object o in each iteration r until o is chosen as the current optimal object.

\begin{lem}
    Given a query q, an object o, two integers m, n and $m \leq n$, then $dcr_{q}^{n}(o)\leq dcr_{q}^{m}(o)$.
    \begin{pot}
        After each iteration r, the value of $cov^r(o,q)$ is comparable or below than the former iteration. Besides, $cd(o,q)$ is constant in each iteration, which results in the decreasing of $dcr^{r}_{q}(o)$. If $m\leq n$, we can safely draw the conclusion that the $dcr_{q}^{n}(o)$ is not larger than $dcr_{q}^{m}(o)$.
    \end{pot}
\end{lem} \label{L2}

Instead of updating all the remaining objects after each iteration, we update the object until it is chosen as the optimal object. Lemma 2 indicates that we can always choose the newly updated object with maximum $dcr^{r}_{q}(o)$ as current optimal object, and objects whose $dcr$ less than $dcr^{r}_{q}(o)$ unnecessary to be updated, which significantly prunes the updating overhead.

We summarize the aforementioned discussion by following three steps to improve the naive version. And we use G to reserve the final solution and RV to record the residuals.
\begin{itemize}
    \item Step 1 (Construct the KMPQ): In this step, we construct the KMPQ for each keyword of $q.\omega$.
    \item Step 2 (Select the optimal object): Select the current optimal object o from all the KMPQs. If o already in G, then continue to select next optimal object. Otherwise, we add o into G if $cov^r(o,q)$ less than RV in all dimensions. Otherwise, we update the $cr(o,q)$ of o and reinsert it into KMPQ.
    \item Step 3 (Iterative step): Repeat Step 2 until RV equals to 0 in all dimensions. Then we take G as our final solution.
\end{itemize}

Algorithm \ref{A3} illustrates the details of MaxMargin algorithm.
%MaxMargin algorithm
\IncMargin{1em}
\begin{algorithm}[!htb]
\caption{$MaxMargin$} \label{A3}   % 给算法一个标签，以便其它地方引用该算法
%\DontPrintSemicolon
\SetKwData{G}{G}\SetKwData{RV}{RV}\SetKwData{indicator}{indicator}
\SetKwData{PQ}{PQ} \SetKwData{maxMargin}{maxMargin}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input {The $KHT$ and the query $q$.}
\Output{A group $G$ of objects as approximation solution.}
\BlankLine
\LinesNumbered
\G$\longleftarrow \emptyset$; \maxMargin $\longleftarrow 0.0$; \indicator $\longleftarrow -1$\;
\For{$i \longleftarrow 0$ \KwTo $|q.\omega|-1$} {
    $RV[i] \longleftarrow q.\theta$\;
    construct the KMPQ $PQ[i]$ for $q.\omega[i]$ with $KHT$\;
}
$r \longleftarrow 0$\;
\While{exists a dimension i satisfies $RV[i]>0.0$} {
    \For{each valid $PQ[j] \in PQ$} {
        $o \longleftarrow PQ[j].top()$\;
        \If{$dcr_q^r(o)>maxMargin$} {
            $maxMargin \longleftarrow dcr_q^r(o)$\;
            $indicator \longleftarrow j$\;
            $maxOID \longleftarrow o$\;
        }
    }
    \If{$maxOID \in G$} {
        $PQ[indicator].dequeue()$\;
        continue\;
    }
    \eIf{all dimensions satisfy $maxOID.cv[i]<RV[i]$} {
        \G $\longleftarrow G\cup \{maxOID\}$\;
        \For{$j \longleftarrow 0$ \KwTo $|q.\omega|-1$} {
            \eIf{$RV[j]\geq maxOID.cv[j]$} {
                $RV[j] \longleftarrow RV[j]-maxOID.cv[j]$\;
            } {
                $RV[j] \longleftarrow 0$\;
                set the $PQ[j]$ to be invalid\;
            }
        }
        $PQ[indicator].dequeue()$\;
        r++\;
    } {
        \For{$j \longleftarrow 0$ \KwTo $|q.\omega|-1$} {
            \If{$RV[j]< maxOID.cv[j]$} {
                $maxOID.cv[j] \longleftarrow RV[j]$\;
            }
        }
        recompute the $dcr_q^r(maxOID)$\;
        $PQ[indicator].dequeue()$\;
        $PQ[indicator].enqueue(maxOID)$\;
    }
}
return $G$ as the final solution\;
\end{algorithm}
\DecMargin{1em}

 MaxMargin utilizes the residual vector RV to record the difference between $q.\theta$ and the coverage weight of G for each query keyword dynamically. Initially, each dimension of RV is set to be $q.\theta$ (line 3). There are two different cases when processing the selected optimal object in each iteration. Case 1: If all dimensions satisfy $maxOID.cv[i]<RV[i]$ (lines 16-25), it needs not to update maxOID and we update RV accordingly. Case 2: Otherwise, we update the dcr of $maxOID$ and the PQ[indicator]. Algorithm terminates until RV equals to 0. Note that, in Algorithm \ref{A3}, we set $PQ[j]$ to invalid if $RV[j]$ is 0 (line 23). Put differently, we prune all the objects in $PQ[j]$ once the $cov(G,q.\omega[j])$ reaches the threshold $q.\theta$. What's more, we update the object $maxOID$ (lines 27-32) only when the coverage weight of o larger than RV in some dimensions instead of updating the remaining $RO_q$ in naive version. Both of these two strategies significantly improve the performance of MaxMargin algorithm.

\begin{figure}
\centering
\includegraphics[width=3.5in, height=3.5in]{maxMargin}
\caption{KMPQ of query q in Table \ref{T2}}
\label{F3}
\end{figure}

\textbf{Example 2}: Let's go back to the KaGWC query q depicted in Table \ref{T2}. As illustrated in Fig. \ref{F3}, we answer the query q in four steps, as follows.
\begin{itemize}
    \item Step 1 (Initial state): In this step, we construct the KMPQ for ``mountain'' and ``temple'', and initiate the $RV=\{0.4,0.4\}$ and $G=\emptyset$ as depicted in Figs. \ref{F3}a and \ref{F3}b.
    \item Step 2 (Fetch $o_3$): Due to the two KMPQs have common optimal object $o_3$, we can select $o_3$ from any of these two KMPQs. And we select $o_3$ from ``mountain''. After fetching $o_3$ from ``moutain'', the KMPQ of ``moutain'' is reorganized and the RV and G are updated as Fig. \ref{F3}c.
    \item Step 3 (Delete $o_3$): As depicted in Figs. \ref{F3}c and \ref{F3}d. After fetching $o_3$, the current optimal object is $o_3$ of ``temple''. However, $o_3$ has been in G. So we delete it from ``temple'' directly, as illustrated in Fig. \ref{F3}f.
    \item Step 4 (Fetch $o_1$): Again, the two KMPQs have common optimal object. We select $o_1$ from ``mountain'', and put it into G. Up to now, RV is equal to 0. We terminate our procedure and return G as the final solution.
\end{itemize}

\begin{thm}
    The approximation ratio of MaxMargin is not larger than $\frac{H(\lfloor cov+1\rfloor)}{q.\theta}$, where $cov$ is the largest $cov(o_j,q)$ for all $o_j \in RO_q$.
    \begin{pot}
        Inspired by the proof in \cite{chvatal1979greedy}, we provide the approximation ratio proof of MaxMargin here. We use m, n to denote the number of elements in $|q.\omega|$ and $|RO_q|$ respectively. We define a $m\times n$ matrix $P=(p_{ij})$ by
        \begin{displaymath}
            p_{ij} = \left \{
                \begin{array}{ll}
                    cw(o_j,q.\omega[i]) & \textrm{if $q.\omega[i] \in o_j.\omega$,}\\
                    0 & \textrm{otherwise.}\\
                \end{array} \right.
        \end{displaymath}

        According to the definition of P, we know that the n columns of P is n coverage weight vectors of n objects. The goal of MaxMargin is to retrieve a group G of objects. And we utilize the incidence vector $x=(x_j)$ to denote the cover set. Clearly, the incidence vector x of an arbitrary cover satisfies:

        \begin{eqnarray*}
            \sum_{j=1}^n p_{ij}x_j \geq q.\theta & for\ all\ i, \\
             x_j \in \{0,1\}  & for \ all \ j.\\
        \end{eqnarray*}

        For ease of presentation, in the following, we refer to the cost distance of $o_j$ as $c_j$. And we claim that these inequations imply
        \begin{equation} \label{E6}
            \sum_{j=1}^{n} H(\lfloor cov_j^1+1\rfloor)c_j x_j\geq q.\theta \sum(c_j: where\ o_j \in G)
        \end{equation}
        for the cover G returned by the greedy heuristic. Once (\ref{E6}) is proved, the theorem will follow by letting x be the incidence vector of an optimal cover.

        To prove (\ref{E6}), it will suffice to exhibit nonnegative numbers $y_1, y_2, ..., y_m$ such that
         \begin{equation} \label{E7}
            \sum_{i=1}^{m}p_{ij}y_{i} \leq H(\sum_{i=1}^{m}p_{ij})c_{j} \quad for\ all\ j
        \end{equation}
        and such that
         \begin{equation} \label{E8}
            \sum_{i=1}^{m} y_{i} = \sum(c_j: where\ o_j \in G)
        \end{equation}
        for then
        \begin{eqnarray*}
            \sum_{j=1}^n H(\sum_{i=1}^m p_{ij})c_j x_j &\geq& \sum_{j=1}^n (\sum_{i=1}^m p_{ij} y_i)x_j\\
            &=& \sum_{i=1}^m(\sum_{j=1}^n p_{ij}x_j)y_i \\
            &\geq& q.\theta \sum_{i=1}^m y_i\\
            &=& q.\theta \sum(c_j: where\ o_j \in G)\\
        \end{eqnarray*}
        as desired.

        The numbers $y_1, y_2, ..., y_m$ satisfying (\ref{E7}) and (\ref{E8}) have a simple intuitive interpretation: each $y_i$ can be interpreted as the cost paid by MaxMargin for covering the keyword $q.\omega[i]$. We use $cov_{j}^{r}$ to denote the coverage weight of query q covered by object $o_j$ at the beginning of iteration r. Without loss of generality, we may assume that G is \{$o_1$,$o_2$,...,$o_r$\}  after r iteration, and so
        \begin{displaymath}
            \frac{cov_{r}^{r}}{c_r} \geq \frac{cov_{j}^{r}}{c_j}
        \end{displaymath}
        for all r and j. If there are t iterations altogether then
        \begin{displaymath}
            \sum(c_j: j \in G)=\sum_{j=1}^{t}c_{j},
        \end{displaymath}
        and
        \begin{displaymath}
            y_{i}=\sum_{r=1}^{t}\frac{c_{r}\cdot cw(o_r, q.\omega[i])}{cov_{r}^{r}}.
        \end{displaymath}
        We know that
        \begin{displaymath}
            \sum_{i=1}^{m}y_i=\sum_{i=1}^{m}\sum_{r=1}^{t}\frac{c_{r}\cdot cw(o_r,  q.\omega[i])}{cov_{r}^{r}}=\sum_{r=1}^{t}c_{r}
        \end{displaymath}

        For any $o_j$ in $RO_q$, we know that the $cov_{j}^{r}$ decrease as the iteration continues. We assume s is the largest superscript such that $cov_{j}^{s}>0$ then
        %\begin{flushleft}
        \begin{eqnarray*}
            \sum_{i=1}^{m}p_{ij}y_{i}&=&\sum_{r=1}^{s}(cov_j^r-cov_j^{r+1})\cdot \frac{c_r}{cov_r^r}\\
            &\leq& c_j\sum_{r=1}^{s}\frac{cov_j^r-cov_j^{r+1}}{cov_j^r}\\
            &=&c_j\sum_{r=1}^s\frac{cov_j^r-cov_j^{r+1}}{cov_j^r}\\
            &\leq& c_j\sum_{r=1}^s\frac{\lfloor cov_j^r + 1\rfloor - \lfloor cov_j^{r+1}\rfloor}{\lfloor cov_j^r + 1\rfloor} \\
            &=&c_j\sum_{r=1}^s \sum_{l=\lfloor cov_j^{r+1} +1\rfloor}^{\lfloor cov_j^r +1\rfloor} \frac{1}{cov_j^r}\\
            &\leq& c_j\sum_{r=1}^s \sum_{l=\lfloor cov_j^{r+1} +1\rfloor}^{\lfloor cov_j^r +1\rfloor} \frac{1}{l}\\
            &=&c_jH(\lfloor cov_j^1 +1\rfloor)-c_jH(\lfloor cov_j^s+1\rfloor)\\
            &\leq& c_jH(\lfloor cov_j^1+1\rfloor)\\
        \end{eqnarray*}
        %\end{flushleft}
    \end{pot}
\end{thm}

